<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="description" content="">
    <meta name="HandheldFriendly" content="True">
    <meta name="description" content="An overview of the steps required to install the Apache Cassandra + Spark stack on Linux.">
    <meta name="keywords" content="cassandra, spark, ">
    <meta property="article:published_time" content="2014-07-15"/>
    <meta property="article:modified_time" content="2014-07-15"/>

     <!-- twitter cards -->
     <meta name="twitter:card" content="summary">
     <meta name="twitter:site" content="@AlTobey">
     <meta name="twitter:title" content="Installing the Cassandra / Spark OSS Stack : @AlTobey Writes">
     <meta name="twitter:creator" content="@AlTobey">
     <meta name="twitter:description" content="An overview of the steps required to install the Apache Cassandra + Spark stack on Linux.">
     <meta name="twitter:domain" content="http://tobert.github.io">

    <title>Installing the Cassandra / Spark OSS Stack</title>
    <link rel="canonical" href="http://tobert.github.io/post/2014-07-15-installing-cassandra-spark-stack.html">
    <link href="http://tobert.github.io/rss.xml" rel="alternate" type="application/rss+xml" title="@AlTobey Writes" />
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap-theme.min.css">
    <link rel="stylesheet" href="/css/tobert.css">
  </head>
<body>

<a href="#content" class="sr-only">Skip to main content</a>

<div class="navbar navbar-default navbar-static-top" role="navigation">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">@AlTobey Writes</a>
    </div>

    <div class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown">Pages <b class="caret"></b></a>
          <ul class="dropdown-menu" role="menu" aria-labelledby="page_menu">
            <li role="presentation"><a role="menuitem" tabindex="-1" href="/pages/japanese-tech-terms.html">Japanese Technical Terms</a></li>
          </ul>
        </li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li><a href="/about.html">About</a></li>
        <li><a href="/contact.html">Contact</a></li>
        <li><a href="https://github.com/tobert/tobert.github.io/edit/master/src/post/installing-cassandra-spark-stack.md">Edit</a></li>
      </ul>
    </div><!--/.nav-collapse -->
  </div>
</div>
<!-- end of header.html -->
<!-- begin container-top -->
<div class="container">
<hr/>
<h1>Installing the Cassandra / Spark OSS Stack</h1>
<hr/>
<br/>
<!-- end container-top -->
<h2>Init</h2>

<p>As mentioned in my <a href="/post/2014-07-14-portacluster-system-imaging.html">portacluster system imaging</a> post,
I am performing this install on 1 admin node (node0) and 6 worker nodes (node[1-6]) running 64-bit Arch Linux.
If you're attempting to do the same on a Redhat or Debian variant, you will need to
change package names and commands for the target distro. The
rest of the process should port over without much modification.</p>

<h2>Overview</h2>

<p>When assembling an analytics stack, there are usually myriad choices to make. For this build, I decided to
build the smallest stack possible that lets me run Spark queries on Cassandra data. As configured it is
not highly available since the Spark master is standalone. (note: Datastax Enterprise Spark's master has
HA based on Cassandra). It's a decent tradeoff for portacluster, since
I can run the master on the admin node which doesn't get rebooted/reimaged constantly. I'm also going to
skip HDFS or some kind of HDFS replacement for now. Options I plan to look at later are GlusterFS's HDFS
adapter and <a href="http://pithos.io">Pithos</a> as an S3 adapter. In the end, the stack is simply Cassandra and
Spark with the <a href="https://github.com/datastax/spark-cassandra-connector">spark-cassandra-connector</a>.</p>

<h2>Responsible Configuration</h2>

<p>For this post I've used my <a href="https://github.com/tobert/perl-ssh-tools">perl-ssh-tools</a> suite. The intent
is to show what needs to be done and one way to do it. For production deployments, I recommend using
your favorite configuration management tool.</p>

<p>perl-ssh-tools uses a configuration similar to dsh, which uses simple files with one
host per line. I use two lists below. Most commands run on the fleet of workers. Because
cl-run.pl provides more than ssh commands, it's also used to run commands on node0 using
its --incl flag e.g. <code>cl-run.pl --list all --incl node0</code>.</p>

<pre><code>cat .dsh/machines.workers
node1
node2
node3
node4
node5
node6
</code></pre>

<p><code>machines.all</code> is the same with node0 added.</p>

<h2>Install Cassandra</h2>

<p>My first pass at this section involved setting up a package repo, but since I don't have time to package
Spark properly right now, I'm going to use the tarball distros of Cassandra and Spark to keep it simple.
<a href="https://github.com/joschi">joschi</a> maintains a package on the <a href="https://aur.archlinux.org/packages/cassandra/">AUR</a>
but I have chosen not to use it for this install.
I'm also using the Arch packages of OpenJDK, which isn't supported by Datastax, but works fine for hacking.
The JDK is pre-installed on my Arch image, it's as simple as <code>sudo pacman -S extra/jdk7-openjdk</code>.</p>

<p>First, I downloaded the Cassandra tarball from <a href="http://cassandra.apache.org/download/">apache.org</a> to
node0 in /srv/public/tgz. Then on the worker nodes, it gets downloaded and expanded in /opt.</p>

<pre><code>pkg=&quot;apache-cassandra-2.0.9-bin.tar.gz&quot;
sudo curl -o /srv/public/tgz/$pkg \
  http://mirrors.gigenet.com/apache/cassandra/2.0.9/apache-cassandra-2.0.9-bin.tar.gz

cl-run.pl --list workers -c &quot;curl http://node0/tgz/$pkg |sudo tar -C /opt -xzf -&quot;
cl-run.pl --list workers -c &quot;sudo ln -s /opt/apache-cassandra-2.0.9 /opt/cassandra&quot;
</code></pre>

<p>To make it easier to do upgrades without regenerating the configuration, I
relocate the conf dir to /etc/cassandra to match what packages do. This assumes there
is no existing /etc/cassandra.</p>

<pre><code>cl-run.pl --list workers -c &quot;sudo mv /opt/cassandra/conf /etc/cassandra&quot;
cl-run.pl --list workers -c &quot;sudo ln -s /etc/cassandra /opt/cassandra/conf&quot;
</code></pre>

<p>I will start Cassandra with a systemd unit, so I push that out as well. This unit
file runs Cassandra out of the tarball as the cassandra user with the stdout/stderr going
to the systemd journal (which you can view with <code>journalctl -f</code>). I also included some
ulimit settings and bump the OOM score downwards to make it less likely that the kernel
will kill Cassandra when out of memory. Since we're going to be running two large JVM apps
on each worker node, this unit also enables cgroups so Cassandra can be given priority
over Spark. Finally, since the target machines have 16GB of RAM, the heap needs to be
set to 8GB (cassandra-env.sh calculates 3995M which is way too low).</p>

<pre><code>cat &gt; cassandra.service &lt;&lt;EOF
[Unit]
Description=Cassandra Tarball
After=network.target

[Service]
User=cassandra
Group=cassandra
RuntimeDirectory=cassandra
PIDFile=/run/cassandra/cassandra.pid
ExecStart=/opt/cassandra/bin/cassandra -f -p /run/cassandra/cassandra.pid
StandardOutput=journal
StandardError=journal
OOMScoreAdjust=-500
LimitNOFILE=infinity
LimitMEMLOCK=infinity
LimitNPROC=infinity
LimitAS=infinity
Environment=MAX_HEAP_SIZE=8G HEAP_NEWSIZE=1G CASSANDRA_HEAPDUMP_DIR=/srv/cassandra/log
CPUAccounting=true
CPUShares=1000

[Install]
WantedBy=multi-user.target
EOF

cl-sendfile.pl --list workers -x -l cassandra.service -r /etc/systemd/system/multi-user.target.wants/cassandra.service
cl-run.pl --list workers -c &quot;sudo systemctl daemon-reload&quot;
</code></pre>

<p>Since all Cassandra data is being redirected to /srv/cassandra and it's going to run as the
cassandra user, those need to be created.</p>

<pre><code>cat &gt; cassandra-user.sh &lt;&lt;EOF
mkdir -p /srv/cassandra/{log,data,commitlogs,saved_caches}
(grep -q '^cassandra:' /etc/group)  || groupadd -g 1234 cassandra
(grep -q '^cassandra:' /etc/passwd) || useradd -u 1234 -c &quot;Apache Cassandra&quot; -g cassandra -s /bin/bash -d /srv/cassandra cassandra
chown -R cassandra:cassandra /srv/cassandra
EOF

cl-run.pl --list workers -x -s cassandra-user.sh
</code></pre>

<h2>Configure Cassandra</h2>

<p>Before starting Cassandra I want to make a few changes to the standard configurations. I'm not a big
fan of LSB so I redirect all of the /var files to /srv/cassandra so they're all in one place. There's
only one SSD in the target systems so the commit log goes on the same drive.</p>

<p>Note that my systems have a bridge in front of the default interface. You will need to adjust the
ip=$() line to work on your systems.</p>

<pre><code>cat cassandra-config.sh
ip=$(ip addr show br0 |perl -ne 'if ($_ =~ /inet (\d+\.\d+\.\d+\.\d+)/) { print $1 }')

perl -ibak -pe &quot;
  s/^(cluster_name:).*/\$1 'Portable Cluster'/;
  s/^(listen|rpc)_address:.*/\${1}_address: $ip/;
  s|/var/lib|/srv|;
  s/(\s+-\s+seeds:).*/\$1 '192.168.10.11,192.168.10.12,192.168.10.13,192.168.10.14,192.168.10.15,192.168.10.16'/
&quot; /opt/cassandra/conf/cassandra.yaml
# EOF

cl-run.pl --list workers -x -s cassandra-config.sh
</code></pre>

<p>The default log4-server.propterties has log4j printing to stdout. This is not desirable in a background
service configuration, so I remove it. The logs are also now written to /srv/cassandra/log.</p>

<pre><code>cat &gt; log4j-server.properties &lt;&lt;EOF
log4j.rootLogger=INFO,R
log4j.appender.R=org.apache.log4j.RollingFileAppender
log4j.appender.R.maxFileSize=20MB
log4j.appender.R.maxBackupIndex=20
log4j.appender.R.layout=org.apache.log4j.PatternLayout
log4j.appender.R.layout.ConversionPattern=%5p [%t] %d{ISO8601} %F (line %L) %m%n
log4j.appender.R.File=/srv/cassandra/log/system.log
log4j.logger.org.apache.thrift.server.TNonblockingServer=ERROR
EOF

cl-sendfile.pl --list workers -x -l log4j-server.properties -r /opt/cassandra/conf/log4j-server.properties
</code></pre>

<p>And with that, Cassandra is ready to start.</p>

<pre><code>cl-run.pl --list workers -c &quot;sudo systemctl start cassandra.service&quot;
ssh node3 tail -f /srv/cassandra/log/system.log
</code></pre>

<h2>Installing Spark</h2>

<p>The process for Spark is quite similar, except that unlike Cassandra, it has a master.</p>

<p>Since I'm not using any Hadoop components, any of the builds should be fine so I used the
hadoop2 build.</p>

<pre><code>pkg=&quot;spark-1.0.1-bin-hadoop2.tgz&quot;
sudo curl -o /srv/public/tgz/$pkg http://d3kbcqa49mib13.cloudfront.net/spark-1.0.1-bin-hadoop2.tgz

cl-run.pl --list all -c &quot;curl http://node0/tgz/$pkg |sudo tar -C /opt -xzf -&quot;
cl-run.pl --list all -c &quot;sudo ln -s /opt/spark-1.0.1-bin-hadoop2 /opt/spark&quot;
cl-run.pl --list all -c &quot;sudo mv /opt/spark/conf /etc/spark&quot;
cl-run.pl --list all -c &quot;sudo ln -s /etc/spark /opt/spark/conf&quot;
</code></pre>

<p>Create /srv/spark and the spark user.</p>

<pre><code>cat &gt; spark-user.sh &lt;&lt;EOF
mkdir -p /srv/spark/{logs,work,tmp,pids}
(grep -q '^spark:' /etc/group)  || groupadd -g 4321 spark
(grep -q '^spark:' /etc/passwd) || useradd -u 4321 -c &quot;Apache Spark&quot; -g spark -s /bin/bash -d /srv/spark spark
chown -R spark:spark /srv/spark
# make spark tmp world writable and sticky
chmod 4755 /srv/spark/tmp
EOF

cl-run.pl --list all -x -s spark-user.sh
</code></pre>

<h2>Configuring Spark</h2>

<p>Many of Spark's settings are controlled by environment variables. Since I want all volatile data
in /srv, many of these need to be changed. Spark will pick up spark-env.sh automatically.</p>

<p>The Intel NUC systems I'm running this stack on have 4 cores and 16G of RAM, so I'll give
Spark 2 cores and 4G of memory for now.</p>

<pre><code>cat &gt; spark-env.sh &lt;&lt;EOF
export SPARK_WORKER_CORES=&quot;2&quot;
export SPARK_WORKER_MEMORY=&quot;4g&quot;
export SPARK_DRIVER_MEMORY=&quot;2g&quot;
export SPARK_REPL_MEM=&quot;4g&quot;
export SPARK_CONF_DIR=&quot;/etc/spark&quot;
export SPARK_TMP_DIR=&quot;/srv/spark/tmp&quot;
export SPARK_PID_DIR=&quot;/srv/spark/pids&quot;
export SPARK_LOG_DIR=&quot;/srv/spark/logs&quot;
export SPARK_WORKER_DIR=&quot;/srv/spark/work&quot;
export SPARK_LOCAL_DIRS=&quot;/srv/spark/tmp&quot;
export SPARK_COMMON_OPTS=&quot;$SPARK_COMMON_OPTS -Dspark.kryoserializer.buffer.mb=32 &quot;
LOG4J=&quot;-Dlog4j.configuration=file://$SPARK_CONF_DIR/log4j.properties&quot;
export SPARK_MASTER_OPTS=&quot; $LOG4J -Dspark.log.file=/srv/spark/logs/master.log &quot;
export SPARK_WORKER_OPTS=&quot; $LOG4J -Dspark.log.file=/srv/spark/logs/worker.log &quot;
export SPARK_EXECUTOR_OPTS=&quot; $LOG4J -Djava.io.tmpdir=/srv/spark/tmp/executor &quot;
export SPARK_REPL_OPTS=&quot; -Djava.io.tmpdir=/srv/spark/tmp/repl/\$USER &quot;
export SPARK_APP_OPTS=&quot; -Djava.io.tmpdir=/srv/spark/tmp/app/\$USER &quot;
export PYSPARK_PYTHON=&quot;/bin/python2&quot;
EOF
</code></pre>

<p>spark-submit and other tools may use spark-defaults.conf to find the master and other configuration items.</p>

<pre><code>cat &gt; spark-defaults.conf &lt;&lt;EOF
spark.master            spark://node0:7077
spark.executor.memory   512m
spark.eventLog.enabled  true
spark.serializer        org.apache.spark.serializer.KryoSerializer
EOF
</code></pre>

<p>The systemd units are a little less complex than Cassandra's. The spark-master.service unit
should only exist on node0, while every other node runs spark-worker. Spark workers are given
a weight of 100 compared to Cassandra's weight of 1000 so that Cassandra is given priority over
Spark without starving it entirely.</p>

<pre><code>cat &gt; spark-worker.service &lt;&lt;EOF
[Unit]
Description=Spark Worker
After=network.target

[Service]
Type=forking
User=spark
Group=spark
ExecStart=/opt/spark/sbin/start-slave.sh 1 spark://node0:7077
StandardOutput=journal
StandardError=journal
LimitNOFILE=infinity
LimitMEMLOCK=infinity
LimitNPROC=infinity
LimitAS=infinity
CPUAccounting=true
CPUShares=100

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>The master unit is similar and only gets installed on node0. Since it is not competing
for resources, there's no need to turn on cgroups for now.</p>

<pre><code>cat &gt; spark-master.service &lt;&lt;EOF
[Unit]
Description=Spark Master
After=network.target

[Service]
Type=forking
User=spark
Group=spark
ExecStart=/opt/spark/sbin/start-master.sh
StandardOutput=journal
StandardError=journal
LimitNOFILE=infinity
LimitMEMLOCK=infinity
LimitNPROC=infinity
LimitAS=infinity

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>Now deploy all of these configs. Relocate the spark config into /etc/spark and copy
a couple templates, then write all the files there. spark-env.sh goes on all nodes.
The unit files are described above. Finally,
a command is run to instruct systemd to read the new unit files.</p>

<pre><code>cl-run.pl --list all -c &quot;sudo cp /opt/spark/conf/log4j.properties.template /opt/spark/conf/log4j.properties&quot;
cl-run.pl --list all -c &quot;sudo cp /opt/spark/conf/fairscheduler.xml.template /opt/spark/conf/fairscheduler.xml&quot;
cl-sendfile.pl --list all -x -l spark-env.sh -r /etc/spark/spark-env.sh
cl-sendfile.pl --list all -x -l spark-defaults.conf -r /etc/spark/spark-defaults.conf
cl-sendfile.pl --list workers -x -l spark-worker.service -r /etc/systemd/system/multi-user.target.wants/spark-worker.service
cl-sendfile.pl --list all --incl node0 -x -l spark-master.service -r /etc/systemd/system/multi-user.target.wants/spark-master.service
cl-run.pl --list all -c &quot;sudo systemctl daemon-reload&quot;
</code></pre>

<p>With all of that done, it's time to turn on Spark to see if it works.</p>

<pre><code>cl-run.pl --list all --incl node0 -c &quot;sudo systemctl start spark-master.service&quot;
cl-run.pl --list workers -c &quot;sudo systemctl start spark-worker.service&quot;
</code></pre>
</div><!-- container-bottom -->
<!-- begin footer.html -->
<footer class="footer">
  <hr style="padding: 1em;"/>
  <div id="footer" class="container">
    
    <a style="padding-left: 1em;" href="https://github.com/tobert/tobert.github.io/commits/master/src/post/installing-cassandra-spark-stack.md">ChangeLog</a>
    
    <a class="pull-right" style="padding-right: 1em;" href="/attributes">License &amp; Credits</a>
  </div>
</footer>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-46953804-1', 'tobert.github.io');
  ga('send', 'pageview');

</script>

</body>
</html>
